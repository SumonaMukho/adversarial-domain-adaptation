data:
    mnist:
        path: /home/arthur/datasets/mnist
        use_inverse: false
    svhn:
        path: /home/arthur/datasets/svhn
train:
    nb_iter: 30000
    batch_size: 64
    gen_opti: adam
    disc_opti: adam
    lr_gen: 2e-4
    lr_disc: 2e-4
    gan_loss: original # {original, wasserstein, lsgan}
    weight_clipping: 0 # if 0 or less, no weight clipping
    nb_iter_d: 1
    nb_iter_g: 1
    vae_init: 0
    loss_weight:
        generator:
            s2s: # source to source
                gan: 1 # GAN generator loss
                vae_rec: 10 # Reconstruction part of the VAE loss (s2s and t2t)
                vae_kl: 10 # KL part of the VAE loss (s2s and t2t)
                cycle: 0 # Cycle-consistency loss (s2s and t2t)
                entropy: 0
                classif_vae: 1 # classification loss on the source reconstruction
            t2t: # target to target
                gan: 1
                vae_rec: 10
                vae_kl: 10
                cycle: 0
            s2t: # source to target
                gan: 1
            t2s: # target to source
                gan: 1
                entropy: 0
        discriminator:
            classif_source: 1 # classification loss on the source input
            s2s:
                gan: 1 # GAN discriminator loss
                r1_reg: 1 # R1 regularization, from (Lars Mescheder et al., 2018)
                feat_matching: 0 # Feature matching loss
            t2t:
                gan: 1 # GAN discriminator loss
                r1_reg: 1 # R1 regularization, from (Lars Mescheder et al., 2018)
                feat_matching: 0 # Feature matching loss
            s2t:
                gan: 1 # GAN discriminator loss
                r1_reg: 1 # R1 regularization, from (Lars Mescheder et al., 2018)
                feat_matching: 0 # Feature matching loss
            t2s:
                gan: 1 # GAN discriminator loss
                r1_reg: 1 # R1 regularization, from (Lars Mescheder et al., 2018)
                feat_matching: 0 # Feature matching loss
    test_every: 10
    save_every: 500
test:
    batch_size: 100
networks:
    generator:
        encoder_name: unit_encoder # name of the encoder function
        decoder_name: unit_decoder # name of the decoder function
        channels: 96
        shared_weights: strong # {weak, strong, none}
    discriminator:
        name: unit_discriminator # name of the discriminator function
        channels: 64
        shared_weights: strong # {weak, strong, none}
results:
    nb_images: 64
        
